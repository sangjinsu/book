# 자연어 처리 바이블

## 자연어처리를 위한 수학

### 2.1 확률의 기초

- 확률은 어떠한 사건이 발생할 수 잇는 가능성을 수치로 나타낸 것이다

#### 2.1.1 확률 변수

- 표본 공간 - 사건이 일어날 수 있는 모든 경우

- 확률 변수 - 어떠한 사건을 실수 표현으로 매칭시키는 함수 

  P(X=사건) = 확률

#### 2.1.2 확률 변수와 확률 분포

- 이산 확률 변수 - 화률 변수 X가 취할 수 있는 값들이 이산적으로 셀 수 있는 경우
- 확률 분포 - 확률 변수가 특정한 값을 가질 확률을 나타내는 함수

- 이산 확률 분포 - 확률 변수가 이산 확률 변수인 경우

  f(x) = P(X=x)

  확률 질량 함수는 이산 확률 변수 X가 임의의 실수 x의 값을 취할 확률을 나타내는 함수

- 연속 확률 변수 - 확률 변수 X가 취할 수 있는 값들이 어떤 범위로 주어지는 경우

- 연속 확률 분포 - 확률 변수가 연속 확률 변수 인 경우

  확률 변수 X가 a와 b 사이에 놓일 확률은 그 구간의 적분을 통해 구할 수 있다

- 확률 밀도 함수 - x에서의 확률이 아니라 상대적인 밀도를 나타낸다

  확률 분포 함수와 마찬가지로 확률의 특성을 가지고 있어 모든 확률 변수의 확률 합은 1이다

#### 2.1.3 조건부 확률

어떤 사상 A가 일어났다고 가정한 상태에서 사상 B 가 일어날 확률이다

P(B|A) = P(A&&B)/P(A)

P(A&&B) = P(A) * P(B|A) = P(B) * P(A|B)

- 사건 A와 B가 독립인 경우

  P(B|A) = P(B)

  P(A&&B) = P(A)*P(B)

  P(A&&B&&C) = P(A)P(B|A)P(C|A&&B)

- [베이즈 정리](https://ko.wikipedia.org/wiki/%EB%B2%A0%EC%9D%B4%EC%A6%88_%EC%A0%95%EB%A6%AC)

#### 2.1.4 기댓값과 분산

기댓값 = 평균과 같은 역할

평균은 X의 확률 분포의 중심 위치에 대한 측도로 사용할 수 있다

분산 - 확률 분포에서 확률 변수들의 퍼져있는 정도

1. 편차 제곱의 평균
2. 제곱의 평균 - 평균의 제곱 

표준 편차 - 분산의 제곱근 

#### 2.1.5 이항분포, 다항분포, 정규분포

이항분포

- 확률이 p인 베르누이 시행을 n번 반복시행할 때 출현 횟수를 나타내는 확률변수 X 의 분포를 의미한다 
- B(n, p), 평균은 np, 분산은 np(1-p)

다항분포

- 이항 분포의 일반화이다, yes, no, I don't know 값이 가능하다
- Multi(P), P = <p1, ...... , Pk>
- 평균 E(xi) = np
- 분산 Var(xi) = npi(1-pi)

정규분포

- 연속 활률 분포 중의 하나로 가우시안 분포라고 불린다
- 정규분표의 표현은 N(평균, 분산)이다 

### 2.2 MLE와 MAP

#### 2.2.1 [Maximum Likelihood Estimation](https://ko.wikipedia.org/wiki/%EC%B5%9C%EB%8C%80%EA%B0%80%EB%8A%A5%EB%8F%84_%EB%B0%A9%EB%B2%95) 

어떤 확률변수에서 표집한 값들을 토대로 그 확률변수의 모수를 구하는 방법이다

어떤 모수가 주어졌을 때, 원하는 값들이 나올 가능도를 최대로 만드는 모수를 선택하는 방법이다

#### 2.2.2 Maximum a Posteriori Estimation

최대 사후 확률은 베이즈 통계학에서 사후 확률의 최빈값을 가리킨다

최대우도에서 어떤 사건이 일어날 확률을 가장 높이는 모수를 찾는 것에 비해, 최대 사후 확률 모수는 모수의 사전 확률과 결합된 확률을 고려한다 

MLE 와 MAP 모두 데이터가 많다면 결과는 다르지 않다. 하지만 데이터가 적다면 사전 지식을 확용한 MAP가 더 유용하다

### 2.3 정보이론과 엔트로피

정보이론은 임의의 정보에 대해 데이터 압축률과 전송률을 최대화할 수 있는 수학적 모델을 제시한다

#### 2.3.1 정보량

1. 중요성 

   어떤 사건이 발생할 가능성이 낮을수록 그 사건은 많은 정보를 가진다

   P(x1) > P(x2) => I(x1) < I(x2)

2. 가법성

   어떤 두 사상 x1, x2 가 독립적이면 

   I(x1x2) = I(x1) + I(x2)

I(x) = 1 / P(x)

#### 2.3.2 [엔트로피](https://ko.wikipedia.org/wiki/%EC%A0%95%EB%B3%B4_%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC)

엔트로피란 확률 변수 X의 표본 공간에서 나타나는 모든 사상들의 정보량의 평균적인 기댓값이다

엔트로피는 정보의 불확실성이 높은지 낮은지 평가하는 지표이다 

- 결합 엔트로피(조건부 엔트로피)는 두 개의 확률 변수에 대한 엔트로피이다 

#### 2.3.3 KL-divergence, Perplexity

- [Kullback-Leiber Drivergence](https://ko.wikipedia.org/wiki/%EC%BF%A8%EB%B0%B1-%EB%9D%BC%EC%9D%B4%EB%B8%94%EB%9F%AC_%EB%B0%9C%EC%82%B0)

  두 분포 P와 Q가 서로 얼마나 일치하는지 측정할 수 있는 수단이다

  결과값이 낮을수록 두 분포가 일치하는 정도가 높다

- Perplexity (PPL)

  두 개의 언어 모델이 있을 때 이를 평가하기 위한 지표로 사용한다

  단어의 수로 정규화된 테스트 데이터에 대한 확률 역수로 계산할 수 있다
